\section{Discussion}

\subsection{Parallelization using the \textit{for} directive}

The Openmp data obtained lead to the same conclusion as the last report: the overhead have to be carefully handled to assure a good use of the processor time.\\

Since we had already data for the pthread implementation of gram, we have compared the data of openmp with the one we get during the pthread lab. Taking pthread has a reference we draw the graph \ref{fig:gram_pthread} page \pageref{fig:gram_pthread}. Which represent the relative speedup of OpenMP with pthread data as reference.

\begin{figure}[ht]
  \begin{center}
         \resizebox{160mm}{!}{\includegraphics{pic/graph_lab2.eps}}
  \end{center}
  \caption{Gram-Schmidt Openmp comparison speedup with pthread}
  \label{fig:gram_pthread}
\end{figure} 

We see that the OpenMP implementation is equivalent to the pthread one.
(if we do not consider matrices of $10\times 10$ wich are not really relevent du to the massive overhead).


\subsection{Parallelization using the {task}

Those results are explained by the overheads related to task creation. Indeed, the master thread create a lot of small tasks for each iteration. Such operation is very time consuming as OpenMP has to create the data environment of the task, put it in the queue and manage the scheduling.  These tasks must also be joined before continuing with the next iteration of the outermost loop. Of course, this synchronization step takes some time, time that is not used efficiently doing some useful computations.\\

This quite big amount of overhead explains why we got those bad results. Indeed, the best speedup achieved is about 2.5, which is not really good.\\

The results' stagnation is due to the fact that passed a certain number of threads, we do not benefit from having more of them because the overhead becomes too important compared to the computation time. So, an increasing speedup cannot be achieved anymore.
